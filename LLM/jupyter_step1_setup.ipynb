# %% [markdown]
"""
# ğŸ¯ VPLE LLM vs Human Intelligence Benchmark
## Step 1: Setup SystÃ¨me Complet

Ce notebook configure tout le systÃ¨me de benchmark pour comparer:
- **LLM**: GÃ©nÃ©ration de scÃ©narios d'attaque via LLaMA 13B + RAG MITRE ATT&CK
- **Human**: Votre expertise (sans rÃ©vÃ©ler vos techniques)
- **Execution**: Vous testez manuellement les scÃ©narios sur VPLE VM

### Workflow de Recherche:
1. **Setup** (ce notebook) - Installation Ollama + RAG
2. **Generation** - LLM gÃ©nÃ¨re scÃ©narios d'attaque  
3. **Manual Testing** - Vous exÃ©cutez sur VPLE VM
4. **Analysis** - Comparaison quantitative des rÃ©sultats

### VPLE Target Information:
- **IP**: 192.168.x.x (ajustez selon votre config)
- **Credentials**: administrator:password
- **Applications**: 7 web apps vulnÃ©rables
- **Access**: Aucun accÃ¨s direct du container - gÃ©nÃ©ration de scÃ©narios uniquement
"""

# %%
import sys
import json
import time
import subprocess
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Import du systÃ¨me de benchmark
try:
    from vple_llm_benchmark import (
        setup_vple_benchmark, 
        generate_vple_scenarios,
        VPLELLMBenchmark
    )
    print("âœ… Modules de benchmark importÃ©s avec succÃ¨s")
except ImportError as e:
    print(f"âŒ Erreur d'import: {e}")
    print("ğŸ”§ Assurez-vous que le fichier vple_llm_benchmark.py est dans le mÃªme rÃ©pertoire")
    sys.exit(1)

print("ğŸ¯ VPLE LLM vs Human Intelligence Benchmark - Setup")
print("=" * 60)

# %%
# Configuration de votre environnement VPLE
VPLE_CONFIG = {
    "ip": "192.168.1.100",  # ğŸ”§ AJUSTEZ SELON VOTRE VPLE VM
    "credentials": {
        "username": "administrator", 
        "password": "password"
    },
    "applications": [
        {"name": "DVWA", "port": 1335, "focus": "Basic web vulnerabilities"},
        {"name": "Mutillidae", "port": 1336, "focus": "OWASP Top 10"}, 
        {"name": "WebGoat", "port": 1337, "focus": "Java-based challenges"},
        {"name": "bWAPP", "port": 8080, "focus": "100+ vulnerabilities"},
        {"name": "Juice Shop", "port": 3000, "focus": "Modern JavaScript"},
        {"name": "Security Ninjas", "port": 8899, "focus": "Real-world scenarios"},
        {"name": "WordPress", "port": 8800, "focus": "CMS vulnerabilities"}
    ]
}

print(f"ğŸ¯ VPLE Target Configuration:")
print(f"   IP: {VPLE_CONFIG['ip']}")
print(f"   Applications: {len(VPLE_CONFIG['applications'])} web apps")
print(f"   Credentials: {VPLE_CONFIG['credentials']['username']}:{VPLE_CONFIG['credentials']['password']}")

# VÃ©rification GPU avant setup
print(f"\nğŸ”¥ VÃ©rification GPU RTX 4090...")
try:
    gpu_check = subprocess.run(["nvidia-smi", "--query-gpu=name,memory.total,memory.used", 
                               "--format=csv,noheader"], 
                              capture_output=True, text=True, timeout=10)
    if gpu_check.returncode == 0:
        gpu_lines = gpu_check.stdout.strip().split('\n')
        for line in gpu_lines:
            if 'RTX 4090' in line or '24' in line:  # 24GB VRAM
                print(f"âœ… GPU DÃ©tectÃ©e: {line}")
                break
        else:
            print(f"âš ï¸ GPU Info: {gpu_lines[0] if gpu_lines else 'Unknown'}")
    else:
        print("âš ï¸ nvidia-smi non disponible - continuons quand mÃªme")
except Exception as e:
    print(f"âš ï¸ Erreur GPU check: {e}")

# %%
"""
ğŸš€ SETUP COMPLET DU SYSTÃˆME DE BENCHMARK

Cette cellule va :
1. Installer Ollama dans le container
2. TÃ©lÃ©charger LLaMA 13B (optimal pour RTX 4090)
3. CrÃ©er la base de connaissances VPLE + MITRE ATT&CK
4. Configurer le systÃ¨me RAG
5. Test de gÃ©nÃ©ration de scÃ©nario

â³ TEMPS ESTIMÃ‰: 15-20 minutes la premiÃ¨re fois
â˜• Parfait pour une pause cafÃ© !
"""

print("ğŸ”§ DÃ‰MARRAGE DU SETUP COMPLET...")
print("â³ Ceci prendra 15-20 minutes la premiÃ¨re fois (tÃ©lÃ©chargement LLaMA 13B)")
print("=" * 70)

setup_start_time = time.time()

# Setup automatique complet
benchmark = setup_vple_benchmark(VPLE_CONFIG["ip"])

setup_duration = time.time() - setup_start_time

if benchmark:
    print(f"\nğŸ‰ SETUP TERMINÃ‰ AVEC SUCCÃˆS!")
    print(f"â±ï¸ Temps total: {setup_duration/60:.1f} minutes")
    
    # VÃ©rification de l'utilisation GPU
    gpu_usage = benchmark.setup.get_gpu_memory_usage()
    if "error" not in gpu_usage:
        print(f"ğŸ“Š GPU Usage: {gpu_usage['used_mb']:.0f}MB / {gpu_usage['total_mb']:.0f}MB ({gpu_usage['usage_percent']:.1f}%)")
    
    print(f"\nâœ… SystÃ¨me prÃªt pour gÃ©nÃ©ration de scÃ©narios!")
    print(f"ğŸ¯ Target: VPLE VM Ã  {VPLE_CONFIG['ip']}")
    
else:
    print("âŒ SETUP Ã‰CHOUÃ‰")
    print("ğŸ” VÃ©rifiez les logs ci-dessus pour diagnostic")
    
print("\n" + "="*70)

# %%
"""
ğŸ§ª TEST RAPIDE DE GÃ‰NÃ‰RATION DE SCÃ‰NARIO

Testons la capacitÃ© du LLM Ã  gÃ©nÃ©rer un scÃ©nario d'attaque
basÃ© sur les informations VPLE dans la base de connaissances.
"""

if 'benchmark' in locals() and benchmark:
    print("ğŸ§ª TEST DE GÃ‰NÃ‰RATION DE SCÃ‰NARIO LLM")
    print("-" * 50)
    
    test_objective = "compromise VPLE web applications to extract sensitive data"
    
    print(f"ğŸ¯ Objectif de test: {test_objective}")
    print(f"ğŸª Target VPLE: {VPLE_CONFIG['ip']}")
    print(f"\nğŸ¤– LLM gÃ©nÃ¨re le scÃ©nario...")
    
    test_start = time.time()
    
    # GÃ©nÃ©ration d'un scÃ©nario de test
    test_scenario = benchmark.rag_system.generate_attack_scenario(
        test_objective, 
        VPLE_CONFIG["ip"]
    )
    
    test_duration = time.time() - test_start
    
    print(f"\nğŸ“‹ SCÃ‰NARIO GÃ‰NÃ‰RÃ‰:")
    print("-" * 30)
    print(f"Nom: {test_scenario.scenario_name}")
    print(f"Apps ciblÃ©es: {', '.join(test_scenario.target_apps)}")
    print(f"Techniques MITRE: {', '.join(test_scenario.mitre_techniques)}")
    print(f"Confiance LLM: {test_scenario.confidence_score:.2f}/1.0")
    print(f"Temps de gÃ©nÃ©ration: {test_duration:.1f}s")
    
    print(f"\nğŸ”— ChaÃ®ne d'attaque ({len(test_scenario.attack_chain)} Ã©tapes):")
    for i, step in enumerate(test_scenario.attack_chain[:3], 1):  # Show first 3
        print(f"  {i}. {step.get('technique', 'N/A')}: {step.get('description', 'N/A')[:60]}...")
    
    if len(test_scenario.attack_chain) > 3:
        print(f"  ... et {len(test_scenario.attack_chain) - 3} Ã©tapes supplÃ©mentaires")
    
    print(f"\nâœ… Test rÃ©ussi ! Le systÃ¨me est opÃ©rationnel.")
    print(f"ğŸ¯ PrÃªt pour gÃ©nÃ©ration de scÃ©narios de recherche.")
    
else:
    print("âŒ Impossible de tester - systÃ¨me non configurÃ©")

# %%
"""
ğŸ’¾ SAUVEGARDE DE LA CONFIGURATION

Sauvegardons la configuration pour les prochaines sessions
"""

if 'benchmark' in locals() and benchmark:
    
    # Configuration du systÃ¨me pour sauvegarde
    system_config = {
        "setup_completed": True,
        "timestamp": datetime.now().isoformat(),
        "vple_config": VPLE_CONFIG,
        "model_info": {
            "name": benchmark.setup.recommended_model,
            "loaded": benchmark.setup.model_loaded
        },
        "rag_ready": benchmark.rag_system is not None,
        "gpu_info": benchmark.setup.get_gpu_memory_usage()
    }
    
    # Sauvegarde
    config_filename = f"vple_benchmark_config_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(config_filename, 'w') as f:
        json.dump(system_config, f, indent=2)
    
    print(f"ğŸ’¾ Configuration sauvegardÃ©e: {config_filename}")
    print(f"ğŸ“Š Status systÃ¨me:")
    print(f"   âœ… Ollama installÃ©: {benchmark.setup.ollama_installed}")
    print(f"   âœ… ModÃ¨le chargÃ©: {benchmark.setup.model_loaded}")
    print(f"   âœ… RAG configurÃ©: {benchmark.rag_system is not None}")
    
    # Variables pour les prochains notebooks
    print(f"\nğŸ”— Variables disponibles pour Step 2:")
    print(f"   â€¢ benchmark: SystÃ¨me de benchmark configurÃ©")
    print(f"   â€¢ VPLE_CONFIG: Configuration VPLE VM")
    
else:
    print("âŒ Aucune configuration Ã  sauvegarder")

# %%
"""
ğŸ“‹ RÃ‰SUMÃ‰ DU SETUP ET PROCHAINES Ã‰TAPES
"""

print("ğŸŠ SETUP STEP 1 TERMINÃ‰!")
print("=" * 50)

if 'benchmark' in locals() and benchmark:
    print("âœ… COMPOSANTS CONFIGURÃ‰S:")
    print("   ğŸ”§ Ollama + LLaMA 13B installÃ©s")
    print("   ğŸ“š Base de connaissances VPLE + MITRE ATT&CK crÃ©Ã©e")
    print("   ğŸ¤– SystÃ¨me RAG opÃ©rationnel") 
    print("   ğŸ¯ Interface VPLE configurÃ©e")
    
    print(f"\nğŸ“Š RESSOURCES SYSTÃˆME:")
    gpu_info = benchmark.setup.get_gpu_memory_usage()
    if "error" not in gpu_info:
        print(f"   ğŸ”¥ GPU: {gpu_info['used_mb']:.0f}MB/{gpu_info['total_mb']:.0f}MB utilisÃ©s")
        print(f"   ğŸ’¾ VRAM libre: {gpu_info['free_mb']:.0f}MB")
    
    print(f"\nğŸ¯ CIBLE VPLE:")
    print(f"   ğŸ“ IP: {VPLE_CONFIG['ip']}")
    print(f"   ğŸ”‘ Credentials: administrator:password")
    print(f"   ğŸŒ Applications: {len(VPLE_CONFIG['applications'])} web apps")
    
    print(f"\nâ¡ï¸ PROCHAINES Ã‰TAPES:")
    print(f"   ğŸ““ Step 2: GÃ©nÃ©ration de scÃ©narios LLM vs Human")
    print(f"   ğŸ“‹ Step 3: CrÃ©ation de templates de test manuel")
    print(f"   ğŸ§ª Step 4: ExÃ©cution manuelle sur VPLE VM")
    print(f"   ğŸ“Š Step 5: Analyse comparative et rÃ©sultats")
    
    print(f"\nğŸ”¬ OBJECTIF DE RECHERCHE:")
    print(f"   DÃ©montrer quantitativement que l'expertise humaine")
    print(f"   surpasse les LLMs actuels en gÃ©nÃ©ration de scÃ©narios cybersÃ©curitÃ©")
    
else:
    print("âŒ SETUP Ã‰CHOUÃ‰")
    print("ğŸ”§ Relancez ce notebook pour diagnostiquer les problÃ¨mes")

print("\n" + "="*50)
print("ğŸš€ Passez au notebook Step 2 pour gÃ©nÃ©rer les scÃ©narios!")
